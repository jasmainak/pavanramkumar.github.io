<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pavan Ramkumar Research Portfolio</title>

    <!-- Bootstrap Core CSS -->
    <!--<link href="css/bootstrap.min.css" rel="stylesheet"> -->
    <link href="css/bootstrap.css" rel="stylesheet">
    
    <!-- Custom CSS -->
    <link href="css/grayscale.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <i class="fa fa-play-circle"></i>  <span class="light">Home</span>
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publications">Publications</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">
                            Projects
                            <b class="caret"></b>
                        </a>
                        <ul class="dropdown-menu">
                            <li><a tabindex="-1" href="#projects-1">Visual search and FEF</a></a></li>
                            <li><a tabindex="-1" href="#projects-2">Rapid scene catergorization and MEG</a></li>
                            <li><a tabindex="-1" href="#projects-3">Economic tradeoffs in movement chunking</a></li>
                            <li><a tabindex="-1" href="#projects-4">Neural representation of uncertainty in reach planning</a></li>
                            <li><a tabindex="-1" href="#projects-5">Reward coding in premotor cortex</a></li>
                            <li><a tabindex="-1" href="#projects-6">Neural representation of color in active vision</a></li>
                            <li><a tabindex="-1" href="#projects-7">V4 neurophysiology with deep networks</a></li>
                        </ul>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h4 class="brand-heading">Bridging Marr's Levels of Analysis</h4>
                        <p class="intro-text">Computational and mechanistic characterizations of neural systems.</p>
                        <a href="#about" class="btn btn-circle page-scroll">
                            <i class="fa fa-angle-double-down animated"></i>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Research Interests</h2>
                <p>The long-term goal of my research is to reveal (1) the motivations underlying our behaviors, (2) the
                strategies we use to achieve them, and (3) the biophysical mechanisms that enable these strategies.</p>
                <p>I specialize in computational modeling of behavior and neural data analysis, bringing insights from 
                electrical engineering, machine learning, and statistics. I am experienced in some of the most widespread 
                techniques — primate neurophysiology, human electromagnetic imaging
                (EEG/MEG), and functional MRI — deployed to study neural systems across scales of ~100 to
                ~1,000,000 neurons. I have applied these tools to study the somatosensory, visual, eye-movement,
                motor control, and reward systems in the brain.</p>
            </div>
        </div>
    </section>
    
    <!-- Publications Section -->
    <section id="publications" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>In Preparation</h2>
                <p>[16] <span style="color: #2ca25f">Ramkumar P</span>, Acuna DE, Berniker M, Grafton S, Turner RS, Körding KP. Optimization costs underlying movement sequence chunking in basal ganglia. </p>
                <p>[15] <span style="color: #2ca25f">Ramkumar P</span>, Fernandes HL, Smith MA, Körding KP. Hue tuning during active vision in natural scenes. </p>
                <p>[14] <span style="color: #2ca25f">Ramkumar P</span>, Cooler S, Dekleva BM, Miller EL, Körding KP. A reinforcement signal in motor and premotor cortices. </p>

                <h2>Under Review</h2>
                <p>[13] Glaser JI*, Wood DW*, Lawlor PN, <span style="color: #2ca25f">Ramkumar P</span>, Körding KP, Segraves MA. Frontal eye field represents expected reward of saccades during natural scene search. </p>
                <p>[12] Dekleva BM, <span style="color: #2ca25f">Ramkumar P</span>, Wanda PA, Körding KP, Miller LE. Uncertainty leads to persistent representations of alternative movements in PMd. </p>

                <h2>Under Revision</h2>
                <p>[11] <span style="color: #2ca25f">Ramkumar P</span>, Acuna DE, Berniker M, Grafton S, Turner RS, Körding KP. Chunking as the result of an efficiency–computation tradeoff. <span style="color: #2ca25f">Nature Communications</span>.</p>
                <p>[10] <span style="color: #2ca25f">Ramkumar P</span>, Hansen BC, Pannasch S, Loschky LC. Visual information representation and natural scene categorization are simultaneous across cortex: An MEG study. <span style="color: #2ca25f">Neuroimage</span>.</p>
                <p>[9] <span style="color: #2ca25f">Ramkumar P*</span>, Lawlor PN*, Glaser JI, Wood DW, Segraves MA, Körding KP. Feature-based attention and spatial selection in frontal eye fields during natural scene search. <span style="color: #2ca25f">Journal of Neurophysiology</span>. </p>
                
                <h2>2015</h2>
                <p>[8] <span style="color: #2ca25f">Ramkumar P</span>, Fernandes HL, Körding KP, Segraves MA. 2015. Modeling peripheral visual acuity enables discovery of gaze strategies at multiple time scales during natural scene search. <span style="color: #2ca25f">Journal of Vision</span>, 15(3):19. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/08-Ramkumar_etal_JVis_2015.pdf">[pdf]</a></p>
                
                <h2>2014</h2>
                <p>[7] <span style="color: #2ca25f">Ramkumar P</span>, Parkkonen L, Hyvärinen A. 2014. Group-level spatial independent component analysis of Fourier envelopes of resting-state MEG data. <span style="color: #2ca25f">Neuroimage</span>, 86:480–491. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/07-Ramkumar_etal_Neuroimage_2014.pdf">[pdf]</a></p>
                
                <h2>2013</h2>
                <p>[6] <span style="color: #2ca25f">Ramkumar P</span>, Jas M, Pannasch S, Parkkonen L, Hari R. 2013. Feature-specific information processing precedes concerted activation in human visual cortex. <span style="color: #2ca25f">Journal of Neuroscience</span>, 33: 7691–7699. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/06-Ramkumar_etal_JNeurosci_2013.pdf">[pdf]</a></p>
                <p>[5] Hyvärinen A, <span style="color: #2ca25f">Ramkumar P</span>. 2013. Testing independent component patterns by inter-subject or inter-session consistency. <span style="color: #2ca25f">Frontiers in Human Neuroscience</span>, 7 (94). <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/05-Hyvarinen_Ramkumar_Frontiers_2013.pdf">[pdf]</a></p>

                <h2>2012</h2>
                <p>[4] <span style="color: #2ca25f">Ramkumar P</span>, Parkkonen L, Hari R, Hyvärinen A. 2012. Characterization of neuromagnetic brain rhythms over time scales of minutes using spatial independent component analysis. <span style="color: #2ca25f">Human Brain Mapping</span>, 33: 1648–1662. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/04-Ramkumar_etal_HBM_2012.pdf">[pdf]</a></p>

                <h2>2010</h2>
                <p>[3] Hyvärinen A, <span style="color: #2ca25f">Ramkumar P</span>, Parkkonen L, Hari R. 2010. Independent component analysis of short-time Fourier transforms for spontaneous EEG/MEG analysis. <span style="color: #2ca25f">Neuroimage</span>, 49: 257–271. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/03-Hyvarinen_etal_Neuroimage_2010.pdf">[pdf]</a></p>
                <p>[2] <span style="color: #2ca25f">Ramkumar P</span>, Parkkonen L, Hari R. 2010. Oscillatory Response Function: Towards a parametric model of rhythmic brain activity. <span style="color: #2ca25f">Human Brain Mappping</span>, 31: 820–834. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/02-Ramkumar_etal_HBM_2010.pdf">[pdf]</a></p>
                <p>[1] Malinen S, Vartiainen N, Hlushchuk Y, Koskinen M, <span style="color: #2ca25f">Ramkumar P</span>, Forss N, Kalso E, Hari R. 2010. Aberrant spatiotemporal resting-state brain activation in patients with chronic pain. <span style="color: #2ca25f">Proceedings of the National Academy of Sciences USA</span>, 107: 6493–6497. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/01-Malinen_etal_PNAS_2010&suppl.pdf">[pdf]</a></p> 
            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-1" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Visual search and FEF</h2>
                <h4><span style="color: #2ca25f">With: Hugo Fernandes, Pat Lawlor, Josh Glaser, Daniel Wood, Mark Segraves, Konrad Körding</span></h4>

                <p>To bring objects of interest in the visual environment into focus, we shift our gaze up to three times a second. Deciding where to look next among a large number of alternatives is thus one of the most frequent decisions we make. Why do we look where we look? </p>
                
                <p>Studies have shown that many factors influence the guidance of eye movements. These factors can be generally grouped into task-independent, or bottom-up features, and task-relevant features. Examples of bottom-up features are luminance contrast, color contrast, energy, and saliency; examples of task-specific features are relevance, learned context about target location, factors that are ecologically relevant but not specific to the task such as intrinsic value, and exploratory strategies for minimizing uncertainty. In addition, change in direction between successive saccades and natural statistics of saccade magnitude and direction also enable saccade prediction. Predictive models of eye movements are derived from priority maps comprising one or more of the above factors.</p>
                <img src="figs/FEF_Figure01.png" alt="FEF" width=600 class="img-rounded" >

                <p>How do the visual and oculomotor systems in the brain narrow down their choices of the next saccade target under uncertainty and limited resources? Independently of the computer vision literature, the field of neurobiology of attention and eye movements has produced evidence suggesting that saccade planning relies on a priority map of the visual field. Such maps have been reported in the lateral intra parietal (LIP) cortex, the frontal eye field (FEF), primary visual cortex (V1) and/or ventral visual area V4. </p>

                <img src="figs/FEF_Figure02.png" alt="FEF" width=600 class="img-rounded" >
                <p>In this project, we attempt to unify the above two fields by (i) training monkeys to search for targets in natural scenes while recording population spikes and local field potentials (LFPs) simultaneously from the FEF, (ii) constructing parametric priority maps of the visual field by implementing computational models of gaze behavior, and (iii) seeking evidence for the computation or representation of these priority maps and their components in FEF firing rates. </p>
            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-2" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Rapid Scene Categorization and MEG</h2>
                <h4><span style="color: #2ca25f">With: Sebastian Pannasch, Bruce Hansen, Lester Loschky</span></h4>
                
                <p>To make effective decisions in our environment, our brains must be able to effectively recognize and comprehend real-world scenes. Human are remarkable at recognizing scene categories from the briefest of glimpses (< 20 ms). The holistic information that can be extracted in such short durations has come to be known as scene gist.</p> 
                
                <p>Computational models of scene gist, such as the spatial envelope (SpEn) model, as well as behavioral studies provide useful suggestions for what visual features the brain might use to categorize scenes. However, they do not inform us about when and where in the brain such information is represented and how scene-categorical judgments are made on the basis of these representations.</p>

                <img src="figs/MEGFigure01.png" alt="MEG" width=600 class="img-rounded" >
                <p>Here, we investigate the brain-behavior relationship underlying rapid scene categorization. We use whole-scalp magnetoencephalography (MEG) to track visual scene information flow in the ventral and temporal cortex, using spatially and temporally resolved maps of decoding accuracy. To investigate the time course of visual representation versus behavioral category judgment, we then use neural decoders in concert with decoders based on SpEn features to study errors in behavioral categorization. Using confusion matrices, we tracked how well patterns of errors in neural decoders could be explained by SpEn decoders and behavioral errors. We find that both SpEn decoders and behavioral errors explain unique variance throughout the ventrotemporal cortex, and that their effects are temporally simultaneous and restricted to 100-250 ms after stimulus onset. Thus, during rapid scene categorization, neural processes that ultimately result in behavioral categorization are simultaneous and colocalized with neural processes underlying visual information representation. </p>
                
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-3" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Economic Tradeoffs in Movement Chunking</h2>
                <h4><span style="color: #2ca25f">With: Daniel Acuna, Max Berniker, Rob Turner, Scott Grafton, Konrad Körding</span></h4>

                <p>We routinely execute complex movement sequences with such effortless ease that the cost of planning them optimally is often under-appreciated. When movements are learned from external cues, they start out highly regular, progressively become more varied until the become habitual and more regular again. </p>
                <img src="figs/Chunking_Figure01.png" alt="Chunking" width=600 class="img-rounded" >

                <p> A common facet of many such complex movements is that they tend to be discrete nature, i.e. they are often executed as chunks. Here, we framed movement chunking as the result of a trade-off between the desire to make efficient movements and minimize the computational complexity of optimizing them. We show that monkeys adopt a cost-effective strategy to deal with this tradeoff. By modeling chunks as minimum-jerk trajectories, we found that kinematic sequences are best described as progressively resembling locally optimal trajectories, with optimization occurring within chunks. Thus, the cumulative optimization costs are kept in check over the course of learning.</p> 
                <img src="figs/Chunking_Figure02.png" alt="Chunking" width=600 class="img-rounded" >

                <p>We also record from globus pallidus (GP) — motor output structures in the basal ganglia — and show that GP neurons encode expected complexity of optimizing movements. Optimal control is thus an important driver of movement sequence learning, both behaviorally and neurally. </p>
                <img src="figs/Chunking_Figure03.png" alt="Chunking" width=600 class="img-rounded" >

            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-4" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Uncertainty in reach planning</h2>
                <h4><span style="color: #2ca25f">With: Brian Dekleva, Paul Wanda, Lee Miller, Konrad Körding</span></h4>
                
                <p>Movements in the real world are often planned with uncertain information about where to move. Understanding the role of uncertainty in movement plans is important to improve rehabilitation therapies and brain-based prostheses. Bayesian estimation theory, which combines sources of information in proportion to their uncertainty, predicts movement behavior when uncertainty about subjective beliefs (priors) and sensory observations (likelihoods) is varied. By manipulating sensory uncertainty of the target in each trial of a reaching task, as well as the prior uncertainty of the target location during the task, we show that monkeys' reaches can be predicted using a Bayesian model that weights the different sources of uncertainty appropriately. </p>
                <img src="figs/Uncertainty_Figure01.jpg" alt="FEF" width=600 class="img-rounded" >

                <p> To perform probabilistic inference of this nature, the brain must represent/reflect uncertainty. We asked how sensory uncertainty is represented in population activity in dorsal premotor cortex (PMd) and primary motor cortex (M1) during movement preparation. We found that greater subjective uncertainty, led to incresed firing rates and broad recruitment of neurons in PMd but not M1. This broad recruitment suggests that multiple movement plans are represented in PMd activity until uncertainty-reducing feedback about the target location is received. </p>

            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-5" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Reward signaling in premotor and motor cortices</h2>
                <h4><span style="color: #2ca25f">With: Brian Dekleva, Sam Cooler, Lee Miller, Konrad Körding</span></h4>
                <p>Reward is an important feedback signal for motor learning. How reward information reaches the motor cortex to influence movement planning and execution is unknown. Therefore, we asked whether premotor and motor cortices encode reward. We found a strong and robust encoding of reward outcome in premotor (PMd) and motor (M1) cortices. In particular, neurons increased their firing rate following trials that were not rewarded. We further investigated the nature of this signal and established that it is unlike any previously reported reward signal in the brain. It is unrelated to reward magnitude expectation or prediction error, it is not influenced by history of reward, and it is not modulated by error magnitude. Using generalized linear modeling of spikes we also carefully verified that the signal is not explained away by differences in kinematics or return reach planning activity. Thus, we found a categorical reward signal in PMd and M1 signaling the presence or absence of reward at the end of a goal-directed task.</p>
                <img src="figs/Reward_Figure01.png" alt="Reward" width=600 class="img-rounded" >
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-6" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Natural Color Vision</h2>
                <h4><span style="color: #2ca25f">With: Hugo Fernandes, Matt Smith, Konrad Körding</span></h4>
                <p>Hue is a circular variable</p>
                <img src="figs/V4_Figure01.png" alt="V4" width=600 class="img-rounded" >

                <p>We built a cool GLM </p>
                <img src="figs/V4_Figure02.png" alt="V4" width=600 class="img-rounded" >

                <p>We can predict firing rates with feature-temporal models of hue. </p>
                <img src="figs/V4_Figure03.png" alt="V4" width=600 class="img-rounded" >

            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-7" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Synthetic Neurophysiology with Deep Convolutional Networks</h2>
                <h4><span style="color: #2ca25f">With: Hugo Fernandes, Matt Smith, Konrad Körding</span></h4>
                <p>To bring objects of interest in the visual environment into focus, we shift our gaze up to three times a second.</p>
                <img src="figs/DeepCNN_Figure01.png" alt="V4" width=600 class="img-rounded" >
                
            </div>
        </div>
    </section>
    
    <!-- Contact Section -->
    <section id="contact" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Contact</h2>
                <ul class="list-inline banner-social-buttons">
                    <li>
                        <a href="mailto:pavan.ramkumar@gmail.com" class="btn btn-default btn-lg"><i class="fa fa-twitter fa-fw"></i> <span class="network-name">Email</span></a>
                    </li>
                    <li>
                        <a href="https://twitter.com/desipoika" class="btn btn-default btn-lg"><i class="fa fa-twitter fa-fw"></i> <span class="network-name">Twitter</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/pavanramkumar" class="btn btn-default btn-lg"><i class="fa fa-github fa-fw"></i> <span class="network-name">Github</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/pdfs/PavanRamkumar_CV_Jan_2016.pdf" class="btn btn-default btn-lg"><i class="fa fa-twitter fa-fw"></i> <span class="network-name">Vitæ</span></a>
                    </li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Map Section 
    <div id="map"></div> -->

    <!-- Footer -->
    <footer>
        <div class="container text-center">
            <p>Copyright &copy; Pavan Ramkumar 2016</p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/jquery.easing.min.js"></script>

    <!-- <!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/
    <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script> -->
    
    <!-- Custom Theme JavaScript -->
    <script src="js/grayscale.js"></script>

</body>

</html>
