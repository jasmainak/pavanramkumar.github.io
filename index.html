<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pavan Ramkumar Research Portfolio</title>

    <!-- Bootstrap Core CSS -->
    <!--<link href="css/bootstrap.min.css" rel="stylesheet"> -->
    <link href="css/bootstrap.css" rel="stylesheet">
    
    <!-- Custom CSS -->
    <link href="css/grayscale.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <i class="fa fa-play-circle"></i>  <span class="light">Home</span>
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#publications">Publications</a>
                    </li>
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">
                            Projects
                            <b class="caret"></b>
                        </a>
                        <ul class="dropdown-menu">
                            <li><a tabindex="-1" href="#projects-1">Visual search and FEF</a></a></li>
                            <li><a tabindex="-1" href="#projects-2">Rapid scene catergorization and MEG</a></li>
                            <li><a tabindex="-1" href="#projects-3">Neural representation of uncertainty in reach planning</a></li>
                            <li><a tabindex="-1" href="#projects-4">Economic tradeoffs in movement chunking</a></li>
                            <li><a tabindex="-1" href="#projects-5">Neural representation of color in active vision</a></li>
                            <li><a tabindex="-1" href="#projects-6">V4 neurophysiology with deep networks</a></li>
                        </ul>
                    </li>
                    <li>
                        <a class="page-scroll" href="#contact">Contact</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h4 class="brand-heading">Bridging Marr's Levels of Analysis</h4>
                        <p class="intro-text">Computational and mechanistic characterizations of neural systems.</p>
                        <a href="#about" class="btn btn-circle page-scroll">
                            <i class="fa fa-angle-double-down animated"></i>
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Research Interests</h2>
                <p>The long-term goal of my research is to reveal (1) the motivations underlying our behaviors, (2) the
                strategies we use to achieve them, and (3) the biophysical mechanisms that enable these strategies.</p>
                <p>I specialize in computational modeling of behavior and neural data analysis, bringing insights from 
                electrical engineering, machine learning, and statistics. I am experienced in some of the most widespread 
                techniques — primate neurophysiology, human electromagnetic imaging
                (EEG/MEG), and functional MRI — deployed to study neural systems across scales of ~100 to
                ~1,000,000 neurons. I have applied these tools to study the somatosensory, visual, eye-movement,
                motor control, and reward systems in the brain.</p>
            </div>
        </div>
    </section>
    
    <!-- Publications Section -->
    <section id="publications" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>In Preparation</h2>
                <p>[16] <span style="color: #2ca25f">Ramkumar P</span>, Acuna DE, Berniker M, Grafton S, Turner RS, Körding KP. Optimization costs underlying movement sequence chunking in basal ganglia. </p>
                <p>[15] <span style="color: #2ca25f">Ramkumar P</span>, Fernandes HL, Smith MA, Körding KP. Hue tuning during active vision in natural scenes. </p>
                <p>[14] <span style="color: #2ca25f">Ramkumar P</span>, Cooler S, Dekleva BM, Miller EL, Körding KP. A reinforcement signal in motor and premotor cortices. </p>

                <h2>Under Review</h2>
                <p>[13] Glaser JI*, Wood DW*, Lawlor PN, <span style="color: #2ca25f">Ramkumar P</span>, Körding KP, Segraves MA. Frontal eye field represents expected reward of saccades during natural scene search. </p>
                <p>[12] Dekleva BM, <span style="color: #2ca25f">Ramkumar P</span>, Wanda PA, Körding KP, Miller LE. Uncertainty leads to persistent representations of alternative movements in PMd. </p>

                <h2>Under Revision</h2>
                <p>[11] <span style="color: #2ca25f">Ramkumar P</span>, Acuna DE, Berniker M, Grafton S, Turner RS, Körding KP. Chunking as the result of an efficiency–computation tradeoff. <span style="color: #2ca25f">Nature Communications</span>.</p>
                <p>[10] <span style="color: #2ca25f">Ramkumar P</span>, Hansen BC, Pannasch S, Loschky LC. Visual information representation and natural scene categorization are simultaneous across cortex: An MEG study. <span style="color: #2ca25f">Neuroimage</span>.</p>
                <p>[9] <span style="color: #2ca25f">Ramkumar P*</span>, Lawlor PN*, Glaser JI, Wood DW, Segraves MA, Körding KP. Feature-based attention and spatial selection in frontal eye fields during natural scene search. <span style="color: #2ca25f">Journal of Neurophysiology</span>. </p>
                
                <h2>2015</h2>
                <p>[8] <span style="color: #2ca25f">Ramkumar P</span>, Fernandes HL, Körding KP, Segraves MA. 2015. Modeling peripheral visual acuity enables discovery of gaze strategies at multiple time scales during natural scene search. <span style="color: #2ca25f">Journal of Vision</span>, 15(3):19. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/08-Ramkumar_etal_JVis_2015.pdf">[pdf]</a></p>
                
                <h2>2014</h2>
                <p>[7] <span style="color: #2ca25f">Ramkumar P</span>, Parkkonen L, Hyvärinen A. 2014. Group-level spatial independent component analysis of Fourier envelopes of resting-state MEG data. <span style="color: #2ca25f">Neuroimage</span>, 86:480–491. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/07-Ramkumar_etal_Neuroimage_2014.pdf">[pdf]</a></p>
                
                <h2>2013</h2>
                <p>[6] <span style="color: #2ca25f">Ramkumar P</span>, Jas M, Pannasch S, Parkkonen L, Hari R. 2013. Feature-specific information processing precedes concerted activation in human visual cortex. <span style="color: #2ca25f">Journal of Neuroscience</span>, 33: 7691–7699. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/06-Ramkumar_etal_JNeurosci_2013.pdf">[pdf]</a></p>
                <p>[5] Hyvärinen A, <span style="color: #2ca25f">Ramkumar P</span>. 2013. Testing independent component patterns by inter-subject or inter-session consistency. <span style="color: #2ca25f">Frontiers in Human Neuroscience</span>, 7 (94). <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/05-Hyvarinen_Ramkumar_Frontiers_2013.pdf">[pdf]</a></p>

                <h2>2012</h2>
                <p>[4] <span style="color: #2ca25f">Ramkumar P</span>, Parkkonen L, Hari R, Hyvärinen A. 2012. Characterization of neuromagnetic brain rhythms over time scales of minutes using spatial independent component analysis. <span style="color: #2ca25f">Human Brain Mapping</span>, 33: 1648–1662. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/04-Ramkumar_etal_HBM_2012.pdf">[pdf]</a></p>

                <h2>2010</h2>
                <p>[3] Hyvärinen A, <span style="color: #2ca25f">Ramkumar P</span>, Parkkonen L, Hari R. 2010. Independent component analysis of short-time Fourier transforms for spontaneous EEG/MEG analysis. <span style="color: #2ca25f">Neuroimage</span>, 49: 257–271. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/03-Hyvarinen_etal_Neuroimage_2010.pdf">[pdf]</a></p>
                <p>[2] <span style="color: #2ca25f">Ramkumar P</span>, Parkkonen L, Hari R. 2010. Oscillatory Response Function: Towards a parametric model of rhythmic brain activity. <span style="color: #2ca25f">Human Brain Mappping</span>, 31: 820–834. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/02-Ramkumar_etal_HBM_2010.pdf">[pdf]</a></p>
                <p>[1] Malinen S, Vartiainen N, Hlushchuk Y, Koskinen M, <span style="color: #2ca25f">Ramkumar P</span>, Forss N, Kalso E, Hari R. 2010. Aberrant spatiotemporal resting-state brain activation in patients with chronic pain. <span style="color: #2ca25f">Proceedings of the National Academy of Sciences USA</span>, 107: 6493–6497. <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/tree/master/pdfs/01-Malinen_etal_PNAS_2010&suppl.pdf">[pdf]</a></p> 
            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-1" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Visual search and FEF</h2>
                
                <img src="figs/FEF_Figure01.png" alt="FEF" class="img-rounded" >
                <p>To bring objects of interest in the visual environment into focus, we shift our gaze up to three times a second.
                Deciding where to look next among a large number of alternatives is thus one of the most frequent decisions we make.
                Why do we look where we look? Do we look at objects that pop out and capture our attention (bottom-up saliency)?
                Do we greedily look at things that resemble what we are searching for (top-down relevance)?
                Do we return to objects that we like? 
                Are we guided by learned context (scene gist), manifest for example as looking primarily on the street to find pedestrians? 
                Do we return to locations that help us learn how to search better? Do we dwell on locations that we are maximally 
                uncertain about in order to minimize uncertainty over time? </p>
                
                <p>Over the last decade in the computer vision literature, several such candidate strategies have been operationalized 
                into predictive models of gaze fixation.</p>

                <p>How do the visual and oculomotor systems in the brain narrow down their choices of the next saccade target under 
                uncertainty and limited resources? Independently of the computer vision literature, the field of neurobiology of attention 
                and eye movements has produced evidence suggesting that saccade planning relies on a priority map of the visual field. 
                Such maps have been reported in the lateral intra parietal (LIP) cortex, the frontal eye field (FEF), 
                primary visual cortex (V1) and/or ventral visual area V4. </p>

                <p>In this project, we attempt to unify the above two fields by 
                (i) training macaques in the Segraves Lab to search for targets in natural scenes while recording population spikes 
                and local field potentials (LFPs) simultaneously from the FEF, 
                (ii) constructing parametric priority maps of the visual field by implementing computational models of gaze behavior, and 
                (iii) seeking evidence for the computation or representation of these priority maps and their components in FEF firing rates, 
                population codes, and LFP spectrograms. </p>
            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-2" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Rapid Scene Categorization and MEG</h2>
                <img src="figs/MEGFigure01.png" alt="MEG" width=600 class="img-rounded" >
                <p>To make effective decisions in our environment, our brains must be able to effectively recognize and comprehend real-world scenes. Human are remarkable at recognizing scene categories from the briefest of glimpses (< 20 ms). The holistic information that can be extracted in such short durations has come to be known as scene gist.</p> 
                
                <p>Computational models of scene gist, such as the spatial envelope (SpEn), as well as behavioral studies provide useful suggestions for what visual features the brain might use to categorize scenes. However, they do not inform us about when and where in the brain such information is represented and how scene-categorical judgments are made on the basis of these representations.</p>

                <p>Here, we investigate the brain-behavior relationship underlying rapid scene categorization. We use whole-scalp magnetoencephalography (MEG) to track visual scene information flow in the ventral and temporal cortex, using spatially and temporally resolved maps of decoding accuracy. To investigate the time course of visual representation versus behavioral category judgment, we then use neural decoders in concert with decoders based on SpEn features to study errors in behavioral categorization. Using confusion matrices, we tracked how well patterns of errors in neural decoders could be explained by SpEn decoders and behavioral errors. We find that both SpEn decoders and behavioral errors explain unique variance throughout the ventrotemporal cortex, and that their effects are temporally simultaneous and restricted to 100-250 ms after stimulus onset. Thus, during rapid scene categorization, neural processes that ultimately result in behavioral categorization are simultaneous and colocalized with neural processes underlying visual information representation. </p>
                
            </div>
        </div>
    </section>

    <!-- Projects Section -->
    <section id="projects-3" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Uncertainty in reach planning</h2>
                <p>To bring objects of interest in the visual environment into focus, we shift our gaze up to three times a second.
                Deciding where to look next among a large number of alternatives is thus one of the most frequent decisions we make.
                Why do we look where we look? Do we look at objects that pop out and capture our attention (bottom-up saliency)?
                Do we greedily look at things that resemble what we are searching for (top-down relevance)?
                Do we return to objects that we like? 
                Are we guided by learned context (scene gist), manifest for example as looking primarily on the street to find pedestrians? 
                Do we return to locations that help us learn how to search better? Do we dwell on locations that we are maximally 
                uncertain about in order to minimize uncertainty over time? </p>
                
            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-4" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Movement Chunking</h2>
                <p>To bring objects of interest in the visual environment into focus, we shift our gaze up to three times a second.
                Deciding where to look next among a large number of alternatives is thus one of the most frequent decisions we make.
                Why do we look where we look? Do we look at objects that pop out and capture our attention (bottom-up saliency)?
                Do we greedily look at things that resemble what we are searching for (top-down relevance)?
                Do we return to objects that we like? 
                Are we guided by learned context (scene gist), manifest for example as looking primarily on the street to find pedestrians? 
                Do we return to locations that help us learn how to search better? Do we dwell on locations that we are maximally 
                uncertain about in order to minimize uncertainty over time? </p>
                
            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-5" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Natural Color Vision</h2>
                <p>To bring objects of interest in the visual environment into focus, we shift our gaze up to three times a second.
                Deciding where to look next among a large number of alternatives is thus one of the most frequent decisions we make.
                Why do we look where we look? Do we look at objects that pop out and capture our attention (bottom-up saliency)?
                Do we greedily look at things that resemble what we are searching for (top-down relevance)?
                Do we return to objects that we like? 
                Are we guided by learned context (scene gist), manifest for example as looking primarily on the street to find pedestrians? 
                Do we return to locations that help us learn how to search better? Do we dwell on locations that we are maximally 
                uncertain about in order to minimize uncertainty over time? </p>
                
            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="projects-6" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>V4 and Deep Convolutional Neural Networks</h2>
                <p>To bring objects of interest in the visual environment into focus, we shift our gaze up to three times a second.
                Deciding where to look next among a large number of alternatives is thus one of the most frequent decisions we make.
                Why do we look where we look? Do we look at objects that pop out and capture our attention (bottom-up saliency)?
                Do we greedily look at things that resemble what we are searching for (top-down relevance)?
                Do we return to objects that we like? 
                Are we guided by learned context (scene gist), manifest for example as looking primarily on the street to find pedestrians? 
                Do we return to locations that help us learn how to search better? Do we dwell on locations that we are maximally 
                uncertain about in order to minimize uncertainty over time? </p>
                
            </div>
        </div>
    </section>
    
    <!-- Contact Section -->
    <section id="contact" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Contact</h2>
                <ul class="list-inline banner-social-buttons">
                    <li>
                        <a href="mailto:pavan.ramkumar@gmail.com" class="btn btn-default btn-lg"><i class="fa fa-twitter fa-fw"></i> <span class="network-name">Email</span></a>
                    </li>
                    <li>
                        <a href="https://twitter.com/desipoika" class="btn btn-default btn-lg"><i class="fa fa-twitter fa-fw"></i> <span class="network-name">Twitter</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/pavanramkumar" class="btn btn-default btn-lg"><i class="fa fa-github fa-fw"></i> <span class="network-name">Github</span></a>
                    </li>
                    <li>
                        <a href="https://github.com/pavanramkumar/pavanramkumar.github.io/pdfs/PavanRamkumar_CV_Jan_2016.pdf" class="btn btn-default btn-lg"><i class="fa fa-twitter fa-fw"></i> <span class="network-name">Vitæ</span></a>
                    </li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Map Section 
    <div id="map"></div> -->

    <!-- Footer -->
    <footer>
        <div class="container text-center">
            <p>Copyright &copy; Pavan Ramkumar 2016</p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="js/jquery.easing.min.js"></script>

    <!-- <!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/
    <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script> -->
    
    <!-- Custom Theme JavaScript -->
    <script src="js/grayscale.js"></script>

</body>

</html>
